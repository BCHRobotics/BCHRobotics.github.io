{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"2386 Trojans Programming Documentation Welcome! Mostly, there are a bunch of guides here for doing different FRC-related things, from setting up a new robot project to advanced game piece detection algorithms. The hope is that all of this information can serve as a quick reference for any FRC programmer - on the Trojans or otherwise. Hopefully you'll find what you need.","title":"Home"},{"location":"#2386-trojans-programming-documentation","text":"Welcome! Mostly, there are a bunch of guides here for doing different FRC-related things, from setting up a new robot project to advanced game piece detection algorithms. The hope is that all of this information can serve as a quick reference for any FRC programmer - on the Trojans or otherwise. Hopefully you'll find what you need.","title":"2386 Trojans Programming Documentation"},{"location":"comp-checklist/","text":"Competition Checklist All of these items should be either brought with the driveteam or loaded with the pit. This is a general list, so it might be missing some more specific items. Feel free to update as needed. programming toolbox (the silver one) controllers (two, ideally with another to act as a spare) at least one LONG ethernet cable at least one, preferrably long, USB-A -> USB-C cable for SparkMAXs the driverstation laptop","title":"Competition Checklist"},{"location":"comp-checklist/#competition-checklist","text":"All of these items should be either brought with the driveteam or loaded with the pit. This is a general list, so it might be missing some more specific items. Feel free to update as needed. programming toolbox (the silver one) controllers (two, ideally with another to act as a spare) at least one LONG ethernet cable at least one, preferrably long, USB-A -> USB-C cable for SparkMAXs the driverstation laptop","title":"Competition Checklist"},{"location":"g_advanced-od/","text":"More Advanced Object Detection Strategies in progress Note that all of these approaches rely on having an accurate way of identifying objects. I would not recommend trying any of them unless you have a good neural network - you will not be able to get very far with color banding.","title":"Advanced Approaches"},{"location":"g_advanced-od/#more-advanced-object-detection-strategies","text":"in progress Note that all of these approaches rely on having an accurate way of identifying objects. I would not recommend trying any of them unless you have a good neural network - you will not be able to get very far with color banding.","title":"More Advanced Object Detection Strategies"},{"location":"g_apriltags/","text":"Apriltags","title":"Overview"},{"location":"g_apriltags/#apriltags","text":"","title":"Apriltags"},{"location":"g_basics-overview/","text":"The Basics of FRC Robotics Programming A note from Maximilian M (aka. 10tothe6), ex-programming lead for the 2386 Trojans: This subset of guides aims to explain everything you need to do to at least get an FRC robot running . I like to think of it as a quick reference of sorts. I won't pretend that this guide is complete - or even that extensive - and I may have forgotten some things. For a very extensive guide on setting up an FRC robot, see this Youtube series . It'll take more time to go through, but I highly recommend it - especially if it's your first time doing anything like this. If you were reading something on this website and had a question about it, for example you thought I worded something weirdly or you think I wrote something wrong, email me at maximilianmcdiarmid@gmail.com and I'll try and help. If the question is more about FRC itself, you're better off looking it up on the internet.","title":"Overview"},{"location":"g_basics-overview/#the-basics-of-frc-robotics-programming","text":"","title":"The Basics of FRC Robotics Programming"},{"location":"g_basics-overview/#a-note-from-maximilian-m-aka-10tothe6-ex-programming-lead-for-the-2386-trojans","text":"This subset of guides aims to explain everything you need to do to at least get an FRC robot running . I like to think of it as a quick reference of sorts. I won't pretend that this guide is complete - or even that extensive - and I may have forgotten some things. For a very extensive guide on setting up an FRC robot, see this Youtube series . It'll take more time to go through, but I highly recommend it - especially if it's your first time doing anything like this. If you were reading something on this website and had a question about it, for example you thought I worded something weirdly or you think I wrote something wrong, email me at maximilianmcdiarmid@gmail.com and I'll try and help. If the question is more about FRC itself, you're better off looking it up on the internet.","title":"A note from Maximilian M (aka. 10tothe6), ex-programming lead for the 2386 Trojans:"},{"location":"g_color-banding/","text":"Object Detection With Color Banding complete This is the simpler of the two common approaches to object detection. The idea is to take a source image and pick out regions that have a similar color - the color of the game piece we want to look for. This is what the camera sees. The green cross marks the center of the image. By looking for yellow circles in the image, we can detect the ball (the red square is its bounding box). Setup Photonvision has an entire framework for color banding built into it, so it's incredibly easy to set up. First, make sure that the camera pipeline is set to Colored Shape . Then, all the settings you'll need to edit appear below the camera feeds in the Threshold and Contours panels. Here's what that looks like: The relevant parts of the photonvision dashboard. Panel number one, the Threshold panel, allows you to change what colors you're trying to look for. The Threshold panel Here's what each of the options do, more specifically: Hue is, well, the hue of the color that you're looking for. Saturation is the saturation of said color. Value is, interestingly enough, the value of that color. Invert Hue is self explanatory. The last three buttons are important - they automatically set the colors to a spot on the image. Just press SET TO AVERAGE and click on the color you want in the camera feed. If you want to be more specific, press SHRINK RANGE and click on the color you want. Pressing EXPAND RANGE and clicking on another area of the image will change the color range to include the color you clicked on. This way you can very quickly tune what colors you're looking for, instead of trying to guess-and-check the HSV (Hue, Saturation, Value) values. NOTE: If you are using color banding, you will almost certainly need to re-calibrate the colors EVERY DAY at competition! Beware of this. Panel number two, the Contours panel, allows you to change what to do with the white regions in the processed image. You can tell Photonvision to look for square regions, circular regions, and so on - I won't pretend to know what exactly Photonvision is doing behind the scenes to look for these different regions, we can just accept that it works well enough. The Contours panel Here's what each of the options do: Target Orientation does... uh idk Target Sort Tells Photonvision how to decide what's object 0, object 1, and so on. Area Is the range of percent areas that will be considered valid targets. Fullness does... idk Perimeter Is the range of perimeters (measured in pixels) that are considered valid. Speckle Rejection idk Target Shape Is the closest geometric shape to how the game piece you're looking for will APPEAR in the image (balls are circles, cubes would be squares, etc.) The rest of the settings depend on your Target Shape , and are fairly self-explanatory. Flaws Since we are only looking for color and no other characteristics like shape, this approach struggles to differentiate between items of the same color - yellow balls vs. yellow cones, for example. This time, the camera can see two yellow objects: a ball and a cone. As you can see, the result is a lot messier. Keep in mind that we're only trying to detect the ball. Worse is when you accidentally detect field elements or other things in the background that have a similar color to the one you want - or just happen to look that way because of the lighting. It gets much worse when the game piece you're looking for is either red or blue (the balls from 2022 is one example) because many field elements are either red or blue. In short, if you're looking for a red or blue game piece you can abandon all hope of using color banding to find it.","title":"Color Banding"},{"location":"g_color-banding/#object-detection-with-color-banding","text":"complete This is the simpler of the two common approaches to object detection. The idea is to take a source image and pick out regions that have a similar color - the color of the game piece we want to look for. This is what the camera sees. The green cross marks the center of the image. By looking for yellow circles in the image, we can detect the ball (the red square is its bounding box).","title":"Object Detection With Color Banding"},{"location":"g_color-banding/#setup","text":"Photonvision has an entire framework for color banding built into it, so it's incredibly easy to set up. First, make sure that the camera pipeline is set to Colored Shape . Then, all the settings you'll need to edit appear below the camera feeds in the Threshold and Contours panels. Here's what that looks like: The relevant parts of the photonvision dashboard. Panel number one, the Threshold panel, allows you to change what colors you're trying to look for. The Threshold panel Here's what each of the options do, more specifically: Hue is, well, the hue of the color that you're looking for. Saturation is the saturation of said color. Value is, interestingly enough, the value of that color. Invert Hue is self explanatory. The last three buttons are important - they automatically set the colors to a spot on the image. Just press SET TO AVERAGE and click on the color you want in the camera feed. If you want to be more specific, press SHRINK RANGE and click on the color you want. Pressing EXPAND RANGE and clicking on another area of the image will change the color range to include the color you clicked on. This way you can very quickly tune what colors you're looking for, instead of trying to guess-and-check the HSV (Hue, Saturation, Value) values.","title":"Setup"},{"location":"g_color-banding/#note","text":"If you are using color banding, you will almost certainly need to re-calibrate the colors EVERY DAY at competition! Beware of this. Panel number two, the Contours panel, allows you to change what to do with the white regions in the processed image. You can tell Photonvision to look for square regions, circular regions, and so on - I won't pretend to know what exactly Photonvision is doing behind the scenes to look for these different regions, we can just accept that it works well enough. The Contours panel Here's what each of the options do: Target Orientation does... uh idk Target Sort Tells Photonvision how to decide what's object 0, object 1, and so on. Area Is the range of percent areas that will be considered valid targets. Fullness does... idk Perimeter Is the range of perimeters (measured in pixels) that are considered valid. Speckle Rejection idk Target Shape Is the closest geometric shape to how the game piece you're looking for will APPEAR in the image (balls are circles, cubes would be squares, etc.) The rest of the settings depend on your Target Shape , and are fairly self-explanatory.","title":"NOTE:"},{"location":"g_color-banding/#flaws","text":"Since we are only looking for color and no other characteristics like shape, this approach struggles to differentiate between items of the same color - yellow balls vs. yellow cones, for example. This time, the camera can see two yellow objects: a ball and a cone. As you can see, the result is a lot messier. Keep in mind that we're only trying to detect the ball. Worse is when you accidentally detect field elements or other things in the background that have a similar color to the one you want - or just happen to look that way because of the lighting. It gets much worse when the game piece you're looking for is either red or blue (the balls from 2022 is one example) because many field elements are either red or blue. In short, if you're looking for a red or blue game piece you can abandon all hope of using color banding to find it.","title":"Flaws"},{"location":"g_commands/","text":"WPILib Commands","title":"Commands"},{"location":"g_commands/#wpilib-commands","text":"","title":"WPILib Commands"},{"location":"g_comp-preparation/","text":"Preparing for Competition You should read this guide before you actually leave for competition. Things To Remember The robot, when at competition, is NOT wireless! Once you flash the radio the only way you will be able to communicate with the robot is through an ethernet cable.","title":"Preparing"},{"location":"g_comp-preparation/#preparing-for-competition","text":"You should read this guide before you actually leave for competition.","title":"Preparing for Competition"},{"location":"g_comp-preparation/#things-to-remember","text":"The robot, when at competition, is NOT wireless! Once you flash the radio the only way you will be able to communicate with the robot is through an ethernet cable.","title":"Things To Remember"},{"location":"g_conventions/","text":"Common Conventions and Things To Remember There are a few things that are very useful to remember - regardless of whether I actually agree with them - because either REV Robotics, WPILib, or First themselves use them. REV Conventions REV Robotics code will usually have 'm_' as a prefix for most variables (I'm unsure why it's 'm' and not another letter) and 'k_' as a prefix for constant variables. Feel free to change this for our own robot projects, as it's a strange way of writing code. WPILib Conventions All coordinate-related things in WPILib follow this convention: X+ is always forwards, Y+ is always LEFT (not right, as you may expect) and Z+ is always up. More information on coordinates can be found here . For those who are used to working with software like Unity, these coordinates take some getting used to.","title":"Conventions"},{"location":"g_conventions/#common-conventions-and-things-to-remember","text":"There are a few things that are very useful to remember - regardless of whether I actually agree with them - because either REV Robotics, WPILib, or First themselves use them.","title":"Common Conventions and Things To Remember"},{"location":"g_conventions/#rev-conventions","text":"REV Robotics code will usually have 'm_' as a prefix for most variables (I'm unsure why it's 'm' and not another letter) and 'k_' as a prefix for constant variables. Feel free to change this for our own robot projects, as it's a strange way of writing code.","title":"REV Conventions"},{"location":"g_conventions/#wpilib-conventions","text":"All coordinate-related things in WPILib follow this convention: X+ is always forwards, Y+ is always LEFT (not right, as you may expect) and Z+ is always up. More information on coordinates can be found here . For those who are used to working with software like Unity, these coordinates take some getting used to.","title":"WPILib Conventions"},{"location":"g_drivetools/","text":"Using Drivetools {drivetools logo, stylized blue-and-yellow} Drivetools is the FRC dashboard that we use. A full guide for installing/using Drivetools can be found at [LINK].","title":"Using Drivetools"},{"location":"g_drivetools/#using-drivetools","text":"{drivetools logo, stylized blue-and-yellow} Drivetools is the FRC dashboard that we use. A full guide for installing/using Drivetools can be found at [LINK].","title":"Using Drivetools"},{"location":"g_hardware-config/","text":"Configuring SparkMAXs in progress Like all other REV devices (I think), you can configure SparkMAXs through REV Hardware Client. To do this you'll need to connect a laptop (that has REV Hardware Client installed) to the SparkMAX's USB-C port. For reference: When starting the REV client on a laptop, you'll see this: {image of the hardware client dashboard} The main reasons we would need to plug into a SparkMAX are to change its CAN id, or clearing sticky faults. These are done as follows: {}","title":"Configuring SparkMAXs"},{"location":"g_hardware-config/#configuring-sparkmaxs","text":"in progress Like all other REV devices (I think), you can configure SparkMAXs through REV Hardware Client. To do this you'll need to connect a laptop (that has REV Hardware Client installed) to the SparkMAX's USB-C port. For reference: When starting the REV client on a laptop, you'll see this: {image of the hardware client dashboard} The main reasons we would need to plug into a SparkMAX are to change its CAN id, or clearing sticky faults. These are done as follows: {}","title":"Configuring SparkMAXs"},{"location":"g_neural-nets/","text":"Object Detection Using Neural Networks in progress This is the more complicated of the two approaches, but is substantially more effective. A deeper understanding of neural networks (than what will be explained on this page) can be found here . If you're interested in learning more about neural nets in general, I'd start there. Setup Getting a neural network running on a coprocessor on an FRC robot involves two steps: training and exporting . Exporting varies depending on what device you're using, but training is the same no matter what. Training PhotonVision is built to accept a specific format of neural network called YOLO (which stands for You Only Look Once, more info here and here and here ). 1. Getting Enough Data You will need, for a good model, over 1000 images of the target object. Taking this many pictures by hand 2. Videos To Images So neural networks require images to work, and what we have done is take a great deal of video . We need to turn the videos into images, which in theory can be done by just splitting the video into its frames. There's no need to use every single frame, a better idea would be to use every n frames, for example every 10 or every 5 frames. There are a number of ways to split a video into frames but the one that I tend to use is done through VLC media player. This is nice because it works both on Windows and MacOS. Exporting Depending on what device you're using as your coprocessor, the ai model needs to be in a different file format. All of the file extensions here tend to be very niche (.onnx, .pt, .rknn, etc.). I will explain here how to do things for both an OrangePi and a Rubik Pi 3, just know that while other devices are certainly possible we don't use them and so I can't offer a guide on how to set things up for them. 1. OrangePi The OrangePi runs neural networks using the .rknn format. You first have to export the .pt file you got after training to a .onnx file, then you can export to .rknn . Quick note: All of the information in this section was derived from a single guide, found here . This page pretty much says the same stuff, so I'm giving all credit to the author of that guide. The only reason I'm writing anything at all is so that this website can remain a one-stop-shop, and so that if the original blog ever goes down this can act as a backup. 1. Rubik Pi 3 Bringing The Model Into Photonvision Adding an object detection model to PhotonVision is slightly different depending on what version of PhotonVision you're using. I'll cover the newer system (2025+ I think) here. First, make sure that you have the Object Detection pipeline selected. With the model uploaded, everything you need will be inside the Object Detection panel. The relevant parts of the PhotonVision dashboard. Here is the full Object Detection panel: The Object Detection panel. The options each do the following: Model is a drop-down, allowing you to select either the default model or one that you've uploaded. Confidence is the range of confidence values that will be considered valid. Area is the range of percent area (of the whole image) values that will be considered valid. Ratio (W/H) is range of width-to-height ratios that will be considered valid. Target Orientation tells PhotonVision to look for either horizontal or verticl rectangles. Target Sort tells PhotonVision how to define object 0, object 1, and so on. Largest for example just means the biggest object gets to be 0, the next-biggest 1, etc. Once everything's been done properly, you should see something along these lines: The camera correctly identifies the ball. Flaws TLDR: the ai can mess up, good data takes work It's still possible, especially if you don't have a lot of training data or if your training data is too self-similar, for parts of the image that aren't actually game pieces to be selected just because they have a similar color. Good models take a lot of data, and that can take a while.","title":"Neural Networks"},{"location":"g_neural-nets/#object-detection-using-neural-networks","text":"in progress This is the more complicated of the two approaches, but is substantially more effective. A deeper understanding of neural networks (than what will be explained on this page) can be found here . If you're interested in learning more about neural nets in general, I'd start there.","title":"Object Detection Using Neural Networks"},{"location":"g_neural-nets/#setup","text":"Getting a neural network running on a coprocessor on an FRC robot involves two steps: training and exporting . Exporting varies depending on what device you're using, but training is the same no matter what.","title":"Setup"},{"location":"g_neural-nets/#training","text":"PhotonVision is built to accept a specific format of neural network called YOLO (which stands for You Only Look Once, more info here and here and here ).","title":"Training"},{"location":"g_neural-nets/#1-getting-enough-data","text":"You will need, for a good model, over 1000 images of the target object. Taking this many pictures by hand","title":"1. Getting Enough Data"},{"location":"g_neural-nets/#2-videos-to-images","text":"So neural networks require images to work, and what we have done is take a great deal of video . We need to turn the videos into images, which in theory can be done by just splitting the video into its frames. There's no need to use every single frame, a better idea would be to use every n frames, for example every 10 or every 5 frames. There are a number of ways to split a video into frames but the one that I tend to use is done through VLC media player. This is nice because it works both on Windows and MacOS.","title":"2. Videos To Images"},{"location":"g_neural-nets/#exporting","text":"Depending on what device you're using as your coprocessor, the ai model needs to be in a different file format. All of the file extensions here tend to be very niche (.onnx, .pt, .rknn, etc.). I will explain here how to do things for both an OrangePi and a Rubik Pi 3, just know that while other devices are certainly possible we don't use them and so I can't offer a guide on how to set things up for them.","title":"Exporting"},{"location":"g_neural-nets/#1-orangepi","text":"The OrangePi runs neural networks using the .rknn format. You first have to export the .pt file you got after training to a .onnx file, then you can export to .rknn . Quick note: All of the information in this section was derived from a single guide, found here . This page pretty much says the same stuff, so I'm giving all credit to the author of that guide. The only reason I'm writing anything at all is so that this website can remain a one-stop-shop, and so that if the original blog ever goes down this can act as a backup.","title":"1. OrangePi"},{"location":"g_neural-nets/#1-rubik-pi-3","text":"","title":"1. Rubik Pi 3"},{"location":"g_neural-nets/#bringing-the-model-into-photonvision","text":"Adding an object detection model to PhotonVision is slightly different depending on what version of PhotonVision you're using. I'll cover the newer system (2025+ I think) here. First, make sure that you have the Object Detection pipeline selected. With the model uploaded, everything you need will be inside the Object Detection panel. The relevant parts of the PhotonVision dashboard. Here is the full Object Detection panel: The Object Detection panel. The options each do the following: Model is a drop-down, allowing you to select either the default model or one that you've uploaded. Confidence is the range of confidence values that will be considered valid. Area is the range of percent area (of the whole image) values that will be considered valid. Ratio (W/H) is range of width-to-height ratios that will be considered valid. Target Orientation tells PhotonVision to look for either horizontal or verticl rectangles. Target Sort tells PhotonVision how to define object 0, object 1, and so on. Largest for example just means the biggest object gets to be 0, the next-biggest 1, etc. Once everything's been done properly, you should see something along these lines: The camera correctly identifies the ball.","title":"Bringing The Model Into Photonvision"},{"location":"g_neural-nets/#flaws","text":"TLDR: the ai can mess up, good data takes work It's still possible, especially if you don't have a lot of training data or if your training data is too self-similar, for parts of the image that aren't actually game pieces to be selected just because they have a similar color. Good models take a lot of data, and that can take a while.","title":"Flaws"},{"location":"g_new-project/","text":"Starting A New Robot Project in progress We can (thankfully) start from a swerve template instead of starting from scratch. Since we use the SwerveMAX modules from REV Robotics, we use this template .","title":"Making A Robot Project"},{"location":"g_new-project/#starting-a-new-robot-project","text":"in progress We can (thankfully) start from a swerve template instead of starting from scratch. Since we use the SwerveMAX modules from REV Robotics, we use this template .","title":"Starting A New Robot Project"},{"location":"g_object-detection/","text":"Detecting Game Pieces complete For more advanced teams, depending on the game being played, being able to detect game pieces during a match can be incredibly helpful. There are two widely-used approaches to this: Color Banding and Neural Networks Color Banding See here Neural Networks See here","title":"Overview"},{"location":"g_object-detection/#detecting-game-pieces","text":"complete For more advanced teams, depending on the game being played, being able to detect game pieces during a match can be incredibly helpful. There are two widely-used approaches to this: Color Banding and Neural Networks","title":"Detecting Game Pieces"},{"location":"g_object-detection/#color-banding","text":"See here","title":"Color Banding"},{"location":"g_object-detection/#neural-networks","text":"See here","title":"Neural Networks"},{"location":"g_photon-setup/","text":"","title":"Photonvision Setup"},{"location":"g_pose-estimation/","text":"","title":"Pose Estimation"},{"location":"g_running-code/","text":"Deploying and Running FRC Robot Code Thankfully, WPILib makes the process of uploading (aka. deploying) and running robot code incredibly easy. Here's a quick run-down: Press ctrl+shift+p on your laptop to open the commands menu. Type 'Deploy Robot Code', and click on the command from WPILib. {image of the deploy robot code thing} The command that you need to run. Wait until on, FRC Driverstation, you see the 'Communications' light turn green. That's it! You can now enable. Some Details 'building' code and 'deploying' code are two different things, and thus have two different commands. {image of the two} The 'Deploy Robot Code' command actually runs a build first, before deploying, so there's no need to always run the build command before the deploy command. It'll do that for you. However, sometimes the 'Build Robot Code' command will try and download vendor libraries from the internet (if you don't have a local copy) - and of course you can't do that while connected to the robot network. So if you get this error or a similar one: {image of the error} Then connect to a proper wi-fi network, run a build by itself, then connect back to the robot and deploy. Once it's downloaded the vendor libraries once, it'll have them saved to your computer and you won't need to download them again unless you change them. Also, by default, deploying code will NOT delete old files on the RoboRIO. You do want to delete old files, so that any autos you delete on the laptop side don't stick around on the RIO side, for example - and saving disk space never hurt anyone. Make sure that old files get deleted by setting [] to true: {image of the variable in question}","title":"Running Robot Code"},{"location":"g_running-code/#deploying-and-running-frc-robot-code","text":"Thankfully, WPILib makes the process of uploading (aka. deploying) and running robot code incredibly easy. Here's a quick run-down: Press ctrl+shift+p on your laptop to open the commands menu. Type 'Deploy Robot Code', and click on the command from WPILib. {image of the deploy robot code thing} The command that you need to run. Wait until on, FRC Driverstation, you see the 'Communications' light turn green. That's it! You can now enable.","title":"Deploying and Running FRC Robot Code"},{"location":"g_running-code/#some-details","text":"'building' code and 'deploying' code are two different things, and thus have two different commands. {image of the two} The 'Deploy Robot Code' command actually runs a build first, before deploying, so there's no need to always run the build command before the deploy command. It'll do that for you. However, sometimes the 'Build Robot Code' command will try and download vendor libraries from the internet (if you don't have a local copy) - and of course you can't do that while connected to the robot network. So if you get this error or a similar one: {image of the error} Then connect to a proper wi-fi network, run a build by itself, then connect back to the robot and deploy. Once it's downloaded the vendor libraries once, it'll have them saved to your computer and you won't need to download them again unless you change them. Also, by default, deploying code will NOT delete old files on the RoboRIO. You do want to delete old files, so that any autos you delete on the laptop side don't stick around on the RIO side, for example - and saving disk space never hurt anyone. Make sure that old files get deleted by setting [] to true: {image of the variable in question}","title":"Some Details"},{"location":"g_software-downloads/","text":"","title":"Downloading FRC Software"},{"location":"g_swerve-math/","text":"The Math Behind Swerve Drives I have never used this knowledge during my time on the robotics team - I only did this research to support the development of an FRC robotics simulator. Therefore, you shouldn't feel obliged to read or understand this. It's only for if you want a more in-depth knowledge of how robotics works. If that's you, read on.","title":"Swerve Drive Math"},{"location":"g_swerve-math/#the-math-behind-swerve-drives","text":"I have never used this knowledge during my time on the robotics team - I only did this research to support the development of an FRC robotics simulator. Therefore, you shouldn't feel obliged to read or understand this. It's only for if you want a more in-depth knowledge of how robotics works. If that's you, read on.","title":"The Math Behind Swerve Drives"},{"location":"g_swerve-modules/","text":"","title":"Swerve Modules"},{"location":"g_vision/","text":"Vision for FRC Robots in progress For FRC programming teams, one of the most important and complicated subjects is using cameras to see and react to the field during a match.","title":"Overview"},{"location":"g_vision/#vision-for-frc-robots","text":"in progress For FRC programming teams, one of the most important and complicated subjects is using cameras to see and react to the field during a match.","title":"Vision for FRC Robots"},{"location":"known-issues/","text":"List of Known Problems and Their Solutions last updated 02/17/2026 Hopefully this helps with troubleshooting any random issues that come up. Please add to this if you find any more problems/solutions! Robot suddenly drives full-speed in a direction? (CAREFULLY disable and enable again to see if it keeps driving, it should head in the same direction it was going in.) This is caused by a value of NaN or Infinity being sent to the drivetrain. The easiest way to get NaN is using a divide by zero, so check for any divides in the code and make sure you're not allowing zeros to get through. One of the swerve wheels is dragging or spinning in the wrong direction? Possible fixes: reset the swerve turning encoders (see [LINK]) check for any unplugged encoder or power cables (and plug them back in) If the above don't work, make sure nobody has edited the swerve-handling part of the robot code, especially the following: in Constants.java public static final double kFrontLeftChassisAngularOffset = -Math.PI / 2; public static final double kFrontRightChassisAngularOffset = 0; public static final double kBackLeftChassisAngularOffset = Math.PI; public static final double kBackRightChassisAngularOffset = Math.PI / 2; You may want to test the problematic motors on their own to make sure they work. Photonvision isn't responding? Restarting either photonvision or the coprocessor should fix it. Power-cycle the coprocessor if not. One of the robot's wheels is spinning randomly? Check the encoder cables! The encoder cable from the NEO (driving motor) should be plugged in to the DRIVING SparkMAX's main encoder port, and the encoder cable from the NEO 550 (turning motor) should be plugged in to the TURNING SparkMAX's main encoder port. This will likely fix the issue. If not, then there's a good chance that the SparkMAX is broken (a 'bad' SparkMAX). Swap the Spark, and if that solves the problem then you know the old Spark was broken.","title":"List of Known Issues"},{"location":"known-issues/#list-of-known-problems-and-their-solutions","text":"last updated 02/17/2026 Hopefully this helps with troubleshooting any random issues that come up. Please add to this if you find any more problems/solutions!","title":"List of Known Problems and Their Solutions"},{"location":"known-issues/#robot-suddenly-drives-full-speed-in-a-direction","text":"(CAREFULLY disable and enable again to see if it keeps driving, it should head in the same direction it was going in.) This is caused by a value of NaN or Infinity being sent to the drivetrain. The easiest way to get NaN is using a divide by zero, so check for any divides in the code and make sure you're not allowing zeros to get through.","title":"Robot suddenly drives full-speed in a direction?"},{"location":"known-issues/#one-of-the-swerve-wheels-is-dragging-or-spinning-in-the-wrong-direction","text":"Possible fixes: reset the swerve turning encoders (see [LINK]) check for any unplugged encoder or power cables (and plug them back in) If the above don't work, make sure nobody has edited the swerve-handling part of the robot code, especially the following: in Constants.java public static final double kFrontLeftChassisAngularOffset = -Math.PI / 2; public static final double kFrontRightChassisAngularOffset = 0; public static final double kBackLeftChassisAngularOffset = Math.PI; public static final double kBackRightChassisAngularOffset = Math.PI / 2; You may want to test the problematic motors on their own to make sure they work.","title":"One of the swerve wheels is dragging or spinning in the wrong direction?"},{"location":"known-issues/#photonvision-isnt-responding","text":"Restarting either photonvision or the coprocessor should fix it. Power-cycle the coprocessor if not.","title":"Photonvision isn't responding?"},{"location":"known-issues/#one-of-the-robots-wheels-is-spinning-randomly","text":"Check the encoder cables! The encoder cable from the NEO (driving motor) should be plugged in to the DRIVING SparkMAX's main encoder port, and the encoder cable from the NEO 550 (turning motor) should be plugged in to the TURNING SparkMAX's main encoder port. This will likely fix the issue. If not, then there's a good chance that the SparkMAX is broken (a 'bad' SparkMAX). Swap the Spark, and if that solves the problem then you know the old Spark was broken.","title":"One of the robot's wheels is spinning randomly?"}]}